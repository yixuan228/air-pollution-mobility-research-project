{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 7: Baghdad Congestion Model\n",
    "\n",
    "## README\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook focuses on developing statistical and machine learning models to explain monthly traffic congestion patterns in **Baghdad**, using the **Traffic Congestion Index (TCI)** as a proxy for **economic activity**. \n",
    "Models in this notebook leverage spatial, environmental, and infrastructural variables - most notably NO₂ concentrations - to test whether air pollution levels can meaningfully reflect urban economic dynamics in data-scarce.\n",
    "\n",
    "### Objective\n",
    "\n",
    "1. Evaluate linear and non-linear modelling techniques to explain sub-city TCI in Baghdad.\n",
    "2. Quantify the relationship between NO₂ levels and congestion as a signal of economic throughput.\n",
    "3. Provide interpretable metrics, including **coefficients** and **local elasticities**, to inform policy.\n",
    "\n",
    "\n",
    "### Workflow\n",
    "\n",
    "#### 1. Data Preparation\n",
    "\n",
    "- Monthly aggregation of sub-city level features:\n",
    "  - **Target**: TCI – total monthly Traffic Congestion Index (aggregated seconds of delay)\n",
    "  - **Predictors**:\n",
    "    - NO₂ mean concentrations (µg/m³)\n",
    "    - Road network lengths by type\n",
    "    - Land use shares (built-up, cropland, water bodies, etc.)\n",
    "    - Population & POI counts\n",
    "    - Climate data (surface temperature, night-time lights, etc.)\n",
    "\n",
    "#### 2. Modelling and Evaluation\n",
    "\n",
    "- Linear Family Models:\n",
    "  - Ordinary Least Squares (OLS)\n",
    "  - Log-Log OLS (elasticity form)\n",
    "  - 3rd-degree Polynomial\n",
    "- Regularised and Tree-based Models:\n",
    "  - Lasso Regression\n",
    "  - Random Forest\n",
    "  - LightGBM\n",
    "\n",
    "Each model was tested on the same training/validation/test split. Performance was assessed using **R²** and **RMSE** on the test set to ensure generalisability.\n",
    "\n",
    "\n",
    "### Modelling Results\n",
    "\n",
    "| Model Type         | Target        | RMSE (Test) | R² (Test) | Key Comments |\n",
    "|--------------------|---------------|-------------|-----------|--------------|\n",
    "| OLS (linear)       | Raw TCI       | 9.85M       | **0.80**  | Simple, interpretable, strong generalisation |\n",
    "| Lasso              | Raw TCI       | 9.83M       | 0.797     | Sparse model; confirms dominant drivers |\n",
    "| LightGBM           | Raw TCI       | 9.61M       | 0.807     | Slightly higher R²; interpretability reduced |\n",
    "| Random Forest      | Raw TCI       | 10.37M      | 0.774     | Non-linear, slight overfit |\n",
    "| Neural Network     | Raw TCI       | 12.87M      | 0.653     | Underperformed despite tuning |\n",
    "| Log-Log OLS        | log(TCI)      | 5.06        | –0.01     | Underfits, loses information |\n",
    "| Poly-3 Regression  | Raw TCI       | 59.7M       | –6.47     | Severe overfitting |\n",
    "\n",
    "The **Ordinary Least Squares (OLS)** model using raw TCI as the dependent variable was selected as the final model due to its **excellent out-of-sample R² (~0.80)**, **simplicity**, and **interpretability**. Despite minor performance gains from LightGBM (+0.7 pp R²), the loss of coefficient transparency makes OLS more suitable for public policy analysis.\n",
    "\n",
    "\n",
    "### Interpretation of Best Congestion Model\n",
    "\n",
    "#### Model Formulation\n",
    "\n",
    "The model is specified as:\n",
    "\n",
    "> **TCIᵢ = β₁×NO₂_mean + β₂×POI_count + β₃×LST_day_mean + … + εᵢ**\n",
    "\n",
    "Where each β coefficient represents the marginal contribution of a feature to monthly traffic congestion at the sub-city level.\n",
    "\n",
    "#### NO₂ Coefficient and Elasticity\n",
    "\n",
    "- **β(NO₂_mean) = +53,986,362**  \n",
    "  A one-unit increase in monthly average NO₂ (µg/m³) is associated with an additional **53.9 million TCI units**. This reflects the magnitude of summing congestion time across roads, days, and cells.\n",
    "\n",
    "- **Elasticity (local at mean)**:\n",
    "  Using the formula  \n",
    "  **εₓ,ᵧ = (βₓ × x̄) / ȳ**,  \n",
    "  we find that the elasticity of TCI with respect to NO₂ is **~0.25**.\n",
    "\n",
    "  > A **1% increase in NO₂** concentration corresponds to a **0.25% increase in total monthly congestion**, at average sub-city conditions in Baghdad.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The modelling exercise confirms that **NO₂ is a powerful and interpretable proxy for economic activity** in Baghdad. Its strong, positive, and statistically significant relationship with congestion underscores the link between urban mobility demand and environmental externalities. The use of a transparent OLS model enables straightforward communication of results to policymakers while preserving predictive strength.\n",
    "\n",
    "Ultimately, this model supports the use of **remote-sensed air quality data** as a near-real-time pulse of economic activity in low-data settings. Baghdad’s congestion patterns are **well-explained by NO₂ and POI data**, with OLS explaining approximately **80% of observed variance**—a robust result for urban analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Init: Prepare Packages and Configuration\n",
    "\n",
    "Get current file/repo/data path in local to make sure the following cells run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "SRC_PATH = Path().resolve().parent / \"src\"\n",
    "sys.path.append(str(SRC_PATH))\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "baghdad_df = pd.read_csv(DATA_PATH / \"temp\" / 'baghdad_monthly_adm3.csv')\n",
    "\n",
    "# Features and targets setup\n",
    "features = [\n",
    "    'no2_mean', \n",
    "    \n",
    "    #'no2_lag1', 'no2_neighbor_lag1',\n",
    "\n",
    "    'NTL_mean', 'pop_sum_m', 'road_len',\n",
    "    'poi_count', 'lu_industrial_area',\n",
    "    'lu_commercial_area', 'lu_residential_area',\n",
    "    'non_built_area'\n",
    "\n",
    "    ,'LST_day_mean','lu_retail_area',\n",
    "    'lu_farmland_area',\n",
    "       'lu_farmyard_area', 'road_primary_len', 'road_motorway_len',\n",
    "       'road_trunk_len', 'road_secondary_len', 'road_tertiary_len',\n",
    "       'road_residential_len', 'grassland_a', 'cropland_a', 'built_up_a'\n",
    "    #    ,'snow_a', 'water_bod_a', 'wetland_a', 'sparse_veg_a', 'mangroves_a',\n",
    "    #    'moss_a', 'unclassified_a'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Models y=TCI/road_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Simple linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple LR (TRAIN): RMSE = 5.3420, R² = 0.7162\n",
      "Simple LR (TEST): RMSE = 6.1411, R² = 0.6770\n"
     ]
    }
   ],
   "source": [
    "# Target definition\n",
    "baghdad_df['y1'] = baghdad_df['TCI'] / baghdad_df['road_len']\n",
    "\n",
    "# Train/test split\n",
    "X = baghdad_df[features]\n",
    "y = baghdad_df['y1']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit & predict\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_test  = lr.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "for label, y_true, y_pred in [\n",
    "    ('TRAIN', y_train, y_pred_train),\n",
    "    ('TEST',  y_test,  y_pred_test)\n",
    "]:\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    print(f\"Simple LR ({label}): RMSE = {rmse:.4f}, R² = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Log-log Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log–Log LR (TRAIN): RMSE = 5.0877, R² = 0.1728\n",
      "Log–Log LR (TEST): RMSE = 5.0621, R² = -0.0067\n"
     ]
    }
   ],
   "source": [
    "# Clone and avoid zeros: common practice is to add a small ε = half the minimum positive value\n",
    "df_ll = baghdad_df.copy()\n",
    "epsilon = df_ll[features + ['TCI', 'road_len']].replace(0, np.nan).min().min() / 2\n",
    "\n",
    "# Log transformation\n",
    "for col in features:\n",
    "    df_ll[col] = np.log(df_ll[col].clip(lower=epsilon))\n",
    "df_ll['y2'] = np.log((df_ll['TCI'] / df_ll['road_len']).clip(lower=epsilon))\n",
    "\n",
    "# Split\n",
    "X = df_ll[features]\n",
    "y = df_ll['y2']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit & evaluate\n",
    "lr_ll = LinearRegression().fit(X_train, y_train)\n",
    "for label, X_, y_, model in [\n",
    "    ('TRAIN', X_train, y_train, lr_ll),\n",
    "    ('TEST',  X_test,  y_test,  lr_ll)\n",
    "]:\n",
    "    pred = model.predict(X_)\n",
    "    rmse = np.sqrt(mean_squared_error(y_, pred))\n",
    "    r2   = r2_score(y_, pred)\n",
    "    print(f\"Log–Log LR ({label}): RMSE = {rmse:.4f}, R² = {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Polynomial Dg3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poly LR (deg 3) (TRAIN): RMSE = 2.8633, R² = 0.9185\n",
      "Poly LR (deg 3) (TEST): RMSE = 33.2734, R² = -8.4833\n"
     ]
    }
   ],
   "source": [
    "# Degree‐3 polynomial expansion (no interactions)\n",
    "poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly = poly.fit_transform(baghdad_df[features])\n",
    "\n",
    "# Train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_poly, baghdad_df['y1'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit & evaluate\n",
    "poly_lr = LinearRegression().fit(X_train, y_train)\n",
    "for label, X_, y_, model in [\n",
    "    ('TRAIN', X_train, y_train, poly_lr),\n",
    "    ('TEST',  X_test,  y_test,  poly_lr)\n",
    "]:\n",
    "    pred = model.predict(X_)\n",
    "    rmse = np.sqrt(mean_squared_error(y_, pred))\n",
    "    r2   = r2_score(y_, pred)\n",
    "    print(f\"Poly LR (deg 3) ({label}): RMSE = {rmse:.4f}, R² = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Models y=TCI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Simple linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple LR (TCI) (TRAIN): RMSE = 8548952.1574, R² = 0.7463\n",
      "Simple LR (TCI) (TEST): RMSE = 9846074.6481, R² = 0.7968\n"
     ]
    }
   ],
   "source": [
    "X = baghdad_df[features]\n",
    "y = baghdad_df['TCI']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr2 = LinearRegression().fit(X_train, y_train)\n",
    "for label, y_true, y_pred in [\n",
    "    ('TRAIN', y_train, lr2.predict(X_train)),\n",
    "    ('TEST',  y_test,  lr2.predict(X_test))\n",
    "]:\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    print(f\"Simple LR (TCI) ({label}): RMSE = {rmse:.4f}, R² = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Log-log Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log–Log LR (TCI) (TRAIN): RMSE = 5.8183, R² = 0.1474\n",
      "Log–Log LR (TCI) (TEST): RMSE = 5.7425, R² = -0.0201\n"
     ]
    }
   ],
   "source": [
    "# Add ε and log‐transform\n",
    "df_ll2 = baghdad_df.copy()\n",
    "epsilon = df_ll2[features + ['TCI']].replace(0, np.nan).min().min() / 2\n",
    "for col in features:\n",
    "    df_ll2[col] = np.log(df_ll2[col].clip(lower=epsilon))\n",
    "df_ll2['y5'] = np.log(df_ll2['TCI'].clip(lower=epsilon))\n",
    "\n",
    "X = df_ll2[features]\n",
    "y = df_ll2['y5']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr_ll2 = LinearRegression().fit(X_train, y_train)\n",
    "for label, X_, y_, model in [\n",
    "    ('TRAIN', X_train, y_train, lr_ll2),\n",
    "    ('TEST',  X_test,  y_test,  lr_ll2)\n",
    "]:\n",
    "    pred = model.predict(X_)\n",
    "    rmse = np.sqrt(mean_squared_error(y_, pred))\n",
    "    r2   = r2_score(y_, pred)\n",
    "    print(f\"Log–Log LR (TCI) ({label}): RMSE = {rmse:.4f}, R² = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Polynomial Dg3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poly LR (TCI, deg 3) (TRAIN): RMSE = 2537682.5992, R² = 0.9776\n",
      "Poly LR (TCI, deg 3) (TEST): RMSE = 59692127.6602, R² = -6.4689\n"
     ]
    }
   ],
   "source": [
    "poly2 = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "X_poly2 = poly2.fit_transform(baghdad_df[features])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_poly2, baghdad_df['TCI'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "poly2_lr = LinearRegression().fit(X_train, y_train)\n",
    "for label, X_, y_, model in [\n",
    "    ('TRAIN', X_train, y_train, poly2_lr),\n",
    "    ('TEST',  X_test,  y_test,  poly2_lr)\n",
    "]:\n",
    "    pred = model.predict(X_)\n",
    "    rmse = np.sqrt(mean_squared_error(y_, pred))\n",
    "    r2   = r2_score(y_, pred)\n",
    "    print(f\"Poly LR (TCI, deg 3) ({label}): RMSE = {rmse:.4f}, R² = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Experimentation to Improve Performance\n",
    "\n",
    "Building upon the baseline models, this section focuses on improving predictive accuracy through advanced machine learning techniques. We experiment with:\n",
    "\n",
    "- Lasso Regression for variable selection and regularization,\n",
    "\n",
    "- Random Forest to capture non-linearities and interactions,\n",
    "\n",
    "- LightGBM, a gradient boosting method known for efficiency and accuracy,\n",
    "\n",
    "- And a Neural Network, designed to learn complex feature representations.\n",
    "\n",
    "Each model is evaluated to compare performance gains over linear methods, forming the basis for selecting the best model in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Simple Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal α        : 10.000000\n",
      "Train →  RMSE = 8539696.9768, R² = 0.7469\n",
      "Test  →  RMSE = 9817700.9341, R² = 0.7980\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# ── Suppress warnings ─────────────────────────────────────\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "# ── (Re)define the full feature set & data split ─────────\n",
    "features_full = [\n",
    "    'no2_mean', \n",
    "    #'no2_lag1', 'no2_neighbor_lag1',\n",
    "\n",
    "    'NTL_mean', 'pop_sum_m', 'road_len',\n",
    "    'poi_count', 'lu_industrial_area',\n",
    "    'lu_commercial_area', 'lu_residential_area',\n",
    "    'non_built_area', 'LST_day_mean', 'lu_retail_area',\n",
    "    'lu_farmland_area', 'lu_farmyard_area',\n",
    "    'road_primary_len', 'road_motorway_len',\n",
    "    'road_trunk_len', 'road_secondary_len',\n",
    "    'road_tertiary_len', 'road_residential_len',\n",
    "    'grassland_a', 'cropland_a', 'built_up_a',\n",
    "    'water_bod_a', 'wetland_a'\n",
    "    #,'snow_a'\n",
    "    # ,'sparse_veg_a', 'mangroves_a', 'moss_a',\n",
    "    # 'unclassified_a'\n",
    "]\n",
    "\n",
    "X_full = baghdad_df[features_full]\n",
    "y_full = baghdad_df['TCI']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ── Pipeline & Fit ────────────────────────────────────────\n",
    "lasso_pipeline = Pipeline([\n",
    "    ('scaler',  StandardScaler()),\n",
    "    ('lassocv', LassoCV(\n",
    "        alphas=np.logspace(-4, 1, 50),\n",
    "        cv=5, max_iter=20000, n_jobs=-1, random_state=42\n",
    "    ))\n",
    "])\n",
    "lasso_pipeline.fit(X_tr, y_tr)\n",
    "\n",
    "# ── Metrics ───────────────────────────────────────────────\n",
    "y_tr_pred = lasso_pipeline.predict(X_tr)\n",
    "y_te_pred = lasso_pipeline.predict(X_te)\n",
    "\n",
    "rmse_tr = np.sqrt(mean_squared_error(y_tr, y_tr_pred))\n",
    "r2_tr   = r2_score(y_tr, y_tr_pred)\n",
    "rmse_te = np.sqrt(mean_squared_error(y_te, y_te_pred))\n",
    "r2_te   = r2_score(y_te, y_te_pred)\n",
    "alpha_opt = lasso_pipeline.named_steps['lassocv'].alpha_\n",
    "\n",
    "# ── Clean output ──────────────────────────────────────────\n",
    "print(f\"Optimal α        : {alpha_opt:.6f}\")\n",
    "print(f\"Train →  RMSE = {rmse_tr:.4f}, R² = {r2_tr:.4f}\")\n",
    "print(f\"Test  →  RMSE = {rmse_te:.4f}, R² = {r2_te:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       coefficient\n",
      "feature                           \n",
      "no2_mean              5.619848e+07\n",
      "Intercept             1.706929e+07\n",
      "poi_count             9.290394e+05\n",
      "LST_day_mean         -4.632916e+05\n",
      "lu_retail_area        9.375779e+02\n",
      "NTL_mean             -6.141713e+02\n",
      "lu_farmyard_area     -5.733545e+02\n",
      "road_motorway_len    -3.758322e+02\n",
      "road_trunk_len       -2.649884e+02\n",
      "road_primary_len     -2.065768e+02\n",
      "road_len              5.777523e+01\n",
      "road_residential_len -4.978246e+01\n",
      "road_secondary_len   -4.752816e+01\n",
      "road_tertiary_len    -4.349994e+01\n",
      "wetland_a            -2.753844e+01\n",
      "pop_sum_m             2.219547e+01\n",
      "grassland_a          -3.167750e+00\n",
      "water_bod_a           4.049054e-01\n",
      "cropland_a           -3.158772e-01\n",
      "non_built_area        2.766269e-01\n",
      "lu_commercial_area    2.415114e-01\n",
      "built_up_a           -8.578196e-02\n",
      "lu_industrial_area   -8.480142e-02\n",
      "lu_residential_area  -8.198301e-02\n",
      "lu_farmland_area      6.113320e-03\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract fitted components\n",
    "scaler = lasso_pipeline.named_steps['scaler']\n",
    "lasso  = lasso_pipeline.named_steps['lassocv']\n",
    "\n",
    "# 2. Scaled-space coefficients\n",
    "coef_scaled = lasso.coef_\n",
    "\n",
    "# 3. Back-transform to original units\n",
    "coef_original = coef_scaled / scaler.scale_\n",
    "intercept_original = (\n",
    "    lasso.intercept_\n",
    "    - np.dot(scaler.mean_ / scaler.scale_, coef_scaled)\n",
    ")\n",
    "\n",
    "# 4. Assemble into DataFrame\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': features_full,\n",
    "    'coefficient': coef_original\n",
    "}).set_index('feature')\n",
    "\n",
    "# Add the intercept\n",
    "coef_df.loc['Intercept'] = intercept_original\n",
    "\n",
    "# 5. Sort by magnitude for readability\n",
    "coef_df['abs_coef'] = coef_df['coefficient'].abs()\n",
    "coef_df = coef_df.sort_values('abs_coef', ascending=False).drop(columns='abs_coef')\n",
    "\n",
    "# 6. Print to console\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF (TRAIN) → RMSE = 5791056.7935, R² = 0.8836\n",
      "RF (TEST) → RMSE = 10372546.1159, R² = 0.7745\n",
      "Selected RF params: {'max_depth': None, 'max_features': 0.9, 'min_samples_leaf': 5, 'n_estimators': 605}\n",
      "\n",
      "Top-10 SHAP drivers:\n",
      "road_primary_len       3846821.8062\n",
      "poi_count              2325672.0367\n",
      "non_built_area         1744155.0331\n",
      "cropland_a             1713642.4255\n",
      "LST_day_mean           1264173.3052\n",
      "road_len               1035734.7067\n",
      "road_residential_len    769925.1754\n",
      "lu_residential_area     764448.9765\n",
      "pop_sum_m               753231.9807\n",
      "road_motorway_len       668670.4499\n",
      "\n",
      "Local elasticities for one test sample (top-10):\n",
      "LST_day_mean        -0.9786\n",
      "built_up_a          -0.7177\n",
      "wetland_a           -0.5245\n",
      "water_bod_a          0.1236\n",
      "grassland_a          0.1188\n",
      "NTL_mean            -0.0563\n",
      "cropland_a          -0.0375\n",
      "road_trunk_len      -0.0000\n",
      "pop_sum_m           -0.0000\n",
      "road_motorway_len    0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import shap, warnings, scipy.stats as st\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ── 1) Raw-unit feature matrix & target ────────────────────────────────\n",
    "X_rf = baghdad_df[features_full].copy()\n",
    "y_rf = baghdad_df['TCI'].copy()\n",
    "\n",
    "X_tr_rf, X_te_rf, y_tr_rf, y_te_rf = train_test_split(\n",
    "    X_rf, y_rf, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ── 2) Randomised hyper-tuning (20 draws) ──────────────────────────────\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators'    : st.randint(400, 1001),        # 400–1000 trees\n",
    "    'max_depth'       : [None, 20, 40],\n",
    "    'min_samples_leaf': [1, 3, 5],\n",
    "    'max_features'    : ['sqrt', 0.7, 0.9]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions = param_dist,\n",
    "    n_iter      = 20,\n",
    "    cv          = 5,\n",
    "    scoring     = 'r2',\n",
    "    return_train_score = True,\n",
    "    n_jobs      = -1,\n",
    "    random_state= 42\n",
    ")\n",
    "rs.fit(X_tr_rf, y_tr_rf)\n",
    "best_rf = rs.best_estimator_\n",
    "\n",
    "# ── 3) Evaluation ───────────────────────────────────────────────────────\n",
    "for label, X_, y_ in [('TRAIN', X_tr_rf, y_tr_rf),\n",
    "                      ('TEST',  X_te_rf, y_te_rf)]:\n",
    "    pred = best_rf.predict(X_)\n",
    "    rmse = np.sqrt(mean_squared_error(y_, pred))\n",
    "    r2   = r2_score(y_, pred)\n",
    "    print(f\"RF ({label}) → RMSE = {rmse:.4f}, R² = {r2:.4f}\")\n",
    "\n",
    "print(\"Selected RF params:\", rs.best_params_)\n",
    "\n",
    "# ── 4) SHAP global importances ─────────────────────────────────────────\n",
    "explainer = shap.TreeExplainer(best_rf)\n",
    "shap_vals = explainer.shap_values(X_te_rf, check_additivity=False)\n",
    "shap_df   = pd.Series(np.abs(shap_vals).mean(axis=0),\n",
    "                      index=X_rf.columns,\n",
    "                      name='mean|SHAP|').sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop-10 SHAP drivers:\")\n",
    "print(shap_df.head(10).to_string(float_format='%.4f'))\n",
    "\n",
    "# ── 5) Elasticity approximation (raw units) ────────────────────────────\n",
    "def elasticity(model, x_row, feature, delta=0.01):\n",
    "    \"\"\"\n",
    "    %Δy / %Δx  using finite difference on raw-unit model.\n",
    "    \"\"\"\n",
    "    x_up = x_row.copy()\n",
    "    bump = x_up[feature] * delta if x_up[feature] != 0 else delta\n",
    "    x_up[feature] += bump\n",
    "    y0 = model.predict(x_row.values.reshape(1, -1))[0]\n",
    "    y1 = model.predict(x_up.values.reshape(1, -1))[0]\n",
    "    if y0 == 0:            # guard against div-by-zero\n",
    "        return np.nan\n",
    "    return ((y1 - y0) / y0) / delta   # elasticity formula\n",
    "\n",
    "sample = X_te_rf.iloc[0]\n",
    "elas = {f: elasticity(best_rf, sample, f) for f in X_rf.columns}\n",
    "elas_df = pd.Series(elas, name='elasticity').sort_values(\n",
    "              key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "print(\"\\nLocal elasticities for one test sample (top-10):\")\n",
    "print(elas_df.head(10).to_string(float_format='%.4f'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global elasticity summary (mean, median, 10–90 % deciles)\n",
      "\n",
      "                      mean  median  <lambda>  <lambda>\n",
      "LST_day_mean        3.4976  0.0000   -1.9369   21.0597\n",
      "lu_industrial_area  0.5642  0.0000   -0.0000    0.0000\n",
      "cropland_a          0.5449  0.0000   -1.6741    2.3852\n",
      "water_bod_a         0.3887  0.0000   -0.7366    1.4454\n",
      "non_built_area      0.1529  0.0000   -4.1205    2.0252\n",
      "road_tertiary_len  -0.1477  0.0000   -0.0000    0.0000\n",
      "built_up_a          0.1431  0.0000   -0.6631    0.9639\n",
      "wetland_a           0.1165  0.0000   -0.1189    0.3125\n",
      "pop_sum_m           0.0527  0.0000   -1.2763    0.8835\n",
      "grassland_a         0.0415  0.0000   -0.4009    0.6188\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ##### Global elasticity distribution (RF, raw units)\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def elasticity_matrix(model, X, delta=0.01):\n",
    "    \"\"\"\n",
    "    Vectorised elasticity for an entire sample matrix X.\n",
    "    Returns: array (n_samples, n_features)\n",
    "    ε_ij = ((f(x_i⊕δ_j) – f(x_i)) / f(x_i)) / δ\n",
    "    \"\"\"\n",
    "    y_base = model.predict(X)\n",
    "    X_bump = X.copy()\n",
    "    elas = np.empty_like(X.values, dtype=float)\n",
    "\n",
    "    for j, col in enumerate(X.columns):\n",
    "        bump = X[col].values * delta\n",
    "        bump[bump == 0] = delta        # guard zeros\n",
    "        X_bump[col] = X[col] + bump\n",
    "        y_bump = model.predict(X_bump)\n",
    "        elas[:, j] = ((y_bump - y_base) / y_base) / delta\n",
    "        X_bump[col] = X[col]           # restore\n",
    "\n",
    "    return elas\n",
    "\n",
    "# 1) Compute matrix on the whole test fold\n",
    "E = elasticity_matrix(best_rf, X_te_rf, delta=0.01)   # shape (n_test, n_feat)\n",
    "\n",
    "# 2) Wrap in DataFrame\n",
    "elas_df = pd.DataFrame(E, columns=X_te_rf.columns)\n",
    "\n",
    "# 3) Aggregate statistics\n",
    "summary = (elas_df\n",
    "           .agg(['mean','median',lambda s: s.quantile(0.1),lambda s: s.quantile(0.9)])\n",
    "           .T.rename(columns={'<lambda_0>':'q10','<lambda_1>':'q90'}))\n",
    "\n",
    "# 4) Rank by |mean|\n",
    "summary['abs_mean'] = summary['mean'].abs()\n",
    "summary = summary.sort_values('abs_mean', ascending=False).drop(columns='abs_mean')\n",
    "\n",
    "print(\"Global elasticity summary (mean, median, 10–90 % deciles)\\n\")\n",
    "print(summary.head(10).to_string(float_format='{:.4f}'.format))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 LightGBM experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1482\n",
      "[LightGBM] [Info] Number of data points in the train set: 403, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 11221479.322727\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1482\n",
      "[LightGBM] [Info] Number of data points in the train set: 403, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 11221479.322727\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM (TRAIN) → RMSE = 6813496.5739, R² = 0.8389\n",
      "LightGBM (TEST) → RMSE = 9560138.9439, R² = 0.8084\n",
      "\n",
      "Best LightGBM parameters:\n",
      "{'subsample': 0.8, 'num_leaves': 50, 'n_estimators': 100, 'min_child_samples': 50, 'max_depth': -1, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1) Prepare raw‐unit dataset\n",
    "X_lgb = baghdad_df[features_full].copy()\n",
    "y_lgb = baghdad_df['TCI'].copy()\n",
    "\n",
    "X_tr_lgb, X_te_lgb, y_tr_lgb, y_te_lgb = train_test_split(\n",
    "    X_lgb, y_lgb, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2) Parameter distributions for RandomizedSearch\n",
    "param_dist = {\n",
    "    'num_leaves':        [31, 50, 100],\n",
    "    'max_depth':         [-1, 10, 20],\n",
    "    'learning_rate':     [0.01, 0.05, 0.1],\n",
    "    'n_estimators':      [100, 200, 500],\n",
    "    'min_child_samples': [10, 20, 50],\n",
    "    'subsample':         [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree':  [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "lgb_reg = lgb.LGBMRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "rs_lgb = RandomizedSearchCV(\n",
    "    estimator=lgb_reg,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    return_train_score=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rs_lgb.fit(X_tr_lgb, y_tr_lgb)\n",
    "\n",
    "best_params_lgb = rs_lgb.best_params_\n",
    "\n",
    "# 3) Refit best model on full training set\n",
    "best_lgb = lgb.LGBMRegressor(**best_params_lgb, random_state=42)\n",
    "best_lgb.fit(X_tr_lgb, y_tr_lgb)\n",
    "\n",
    "# 4) Evaluate on train and test\n",
    "for label, X_e, y_e in [('TRAIN', X_tr_lgb, y_tr_lgb), ('TEST', X_te_lgb, y_te_lgb)]:\n",
    "    y_pred = best_lgb.predict(X_e)\n",
    "    rmse   = np.sqrt(mean_squared_error(y_e, y_pred))\n",
    "    r2     = r2_score(y_e, y_pred)\n",
    "    print(f\"LightGBM ({label}) → RMSE = {rmse:.4f}, R² = {r2:.4f}\")\n",
    "\n",
    "print(\"\\nBest LightGBM parameters:\")\n",
    "print(best_params_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Performance Metrics\n",
      "              RMSE     R2\n",
      "TRAIN 6849906.7318 0.8371\n",
      "TEST  9608374.3596 0.8065\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "y_tr_pred = best_lgb.predict(X_tr_lgb)\n",
    "y_te_pred = best_lgb.predict(X_te_lgb)\n",
    "\n",
    "# Compute metrics\n",
    "metrics = pd.DataFrame({\n",
    "    'RMSE': [\n",
    "        np.sqrt(mean_squared_error(y_tr_lgb, y_tr_pred)),\n",
    "        np.sqrt(mean_squared_error(y_te_lgb, y_te_pred))\n",
    "    ],\n",
    "    'R2': [\n",
    "        r2_score(y_tr_lgb, y_tr_pred),\n",
    "        r2_score(y_te_lgb, y_te_pred)\n",
    "    ]\n",
    "}, index=['TRAIN', 'TEST'])\n",
    "\n",
    "# Display nicely\n",
    "print(\"LightGBM Performance Metrics\")\n",
    "print(metrics.to_string(float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 global drivers by SHAP:\n",
      "road_primary_len      4618026.5990\n",
      "poi_count             3937523.5479\n",
      "LST_day_mean          2114417.4322\n",
      "road_motorway_len     1763763.9300\n",
      "non_built_area        1404821.6684\n",
      "pop_sum_m             1159875.8743\n",
      "lu_residential_area   1131794.4888\n",
      "water_bod_a           1121480.6243\n",
      "road_secondary_len    1039664.0811\n",
      "lu_commercial_area     890669.2143\n",
      "\n",
      "Elasticity summary (percent-response per 1% feature bump):\n",
      "                    mean_elas  median_elas     q10    q90\n",
      "LST_day_mean           1.1533       0.0000 -3.8567 7.1198\n",
      "grassland_a           -0.9425       0.0000  0.0000 0.0000\n",
      "road_primary_len      -0.6787       0.0000  0.0000 0.0000\n",
      "water_bod_a           -0.6063       0.0000  0.0000 0.0000\n",
      "cropland_a            -0.3080       0.0000  0.0000 0.0000\n",
      "built_up_a            -0.2184       0.0000  0.0000 0.0000\n",
      "non_built_area        -0.1665       0.0000  0.0000 0.0000\n",
      "lu_industrial_area    -0.1052       0.0000  0.0000 0.0000\n",
      "NTL_mean              -0.1023       0.0000  0.0000 0.0000\n",
      "road_tertiary_len     -0.0630       0.0000  0.0000 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 1) Global SHAP importances\n",
    "explainer = shap.TreeExplainer(best_lgb)\n",
    "shap_vals  = explainer.shap_values(X_te_lgb)\n",
    "# mean absolute impact on model output\n",
    "shap_imp   = pd.Series(np.abs(shap_vals).mean(axis=0),\n",
    "                       index=X_te_lgb.columns,\n",
    "                       name='mean|SHAP|').sort_values(ascending=False)\n",
    "\n",
    "print(\"Top-10 global drivers by SHAP:\")\n",
    "print(shap_imp.head(10).to_string(float_format='%.4f'))\n",
    "\n",
    "# Optional: visual summary\n",
    "# shap.summary_plot(shap_vals, X_te_lgb, plot_type=\"bar\")\n",
    "\n",
    "# 2) Elasticity approximation (finite-difference)\n",
    "def elasticity_pct(model, X, feature, delta=0.01):\n",
    "    \"\"\"\n",
    "    Approximates ε = (%Δy) / (%Δx) for a raw-unit model:\n",
    "      For each row i: bump xi by xi*delta, compute new y,\n",
    "      then (Δy / y) / delta.\n",
    "    Returns vector of local elasticities for each observation.\n",
    "    \"\"\"\n",
    "    x_base = X[feature].values\n",
    "    bump   = np.where(x_base!=0, x_base*delta, delta)\n",
    "    X_up   = X.copy()\n",
    "    X_up[feature] = x_base + bump\n",
    "\n",
    "    y0 = model.predict(X)\n",
    "    y1 = model.predict(X_up)\n",
    "    # avoid /0\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        eps = ((y1 - y0) / y0) / delta\n",
    "    return eps\n",
    "\n",
    "# 3) Compute elasticities for each feature across the test set\n",
    "elas_dict = {}\n",
    "for feat in X_te_lgb.columns:\n",
    "    eps = elasticity_pct(best_lgb, X_te_lgb, feat, delta=0.01)\n",
    "    # summarise: mean, median, 10% & 90% deciles\n",
    "    elas_dict[feat] = [\n",
    "        np.nanmean(eps),\n",
    "        np.nanmedian(eps),\n",
    "        np.nanpercentile(eps, 10),\n",
    "        np.nanpercentile(eps, 90)\n",
    "    ]\n",
    "\n",
    "elas_df = pd.DataFrame.from_dict(\n",
    "    elas_dict,\n",
    "    orient='index',\n",
    "    columns=['mean_elas','median_elas','q10','q90']\n",
    ").sort_values('mean_elas', key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "print(\"\\nElasticity summary (percent-response per 1% feature bump):\")\n",
    "print(elas_df.head(10).to_string(float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_dims': [32, 16], 'dropout': 0.1, 'lr': 0.001} -0.46893307931640615 -0.522192213517753\n",
      "{'hidden_dims': [64, 32], 'dropout': 0.2, 'lr': 0.0005} -0.46893307363275527 -0.5221922030759059\n",
      "\n",
      "Best NN Performance:\n",
      "Train R²=-0.4244, Val R²=-0.4689, Test R²=-0.5222\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1) Prepare & scale data\n",
    "X = baghdad_df[features_full].values\n",
    "y = baghdad_df['TCI'].values.reshape(-1,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_tr, X_tmp, y_tr, y_tmp = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_te, y_val, y_te = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 2) DataLoaders\n",
    "def make_loader(X, y, bs, shuffle=False):\n",
    "    ds = TensorDataset(torch.from_numpy(X).float(),\n",
    "                       torch.from_numpy(y).float())\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=shuffle)\n",
    "\n",
    "# 3) Model with BatchNorm\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims, dropout):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(dims)-1):\n",
    "            layers += [\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.BatchNorm1d(dims[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 4) Training routine with early stopping\n",
    "def train_nn(hidden_dims=[32,16], dropout=0.2, lr=1e-3, wd=1e-4,\n",
    "             batch_size=64, epochs=100, patience=10):\n",
    "    dims = [X_tr.shape[1]] + hidden_dims + [1]\n",
    "    model = MLP(dims, dropout).to(device)\n",
    "    opt   = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    sched = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=10)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    tr_load = make_loader(X_tr, y_tr, batch_size, shuffle=True)\n",
    "    val_load= make_loader(X_val, y_val, batch_size)\n",
    "\n",
    "    best_val_loss, wait = np.inf, 0\n",
    "    for ep in range(epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        for xb,yb in tr_load:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss_fn(model(xb), yb).backward()\n",
    "            opt.step()\n",
    "            sched.step()\n",
    "\n",
    "        # validate\n",
    "        model.eval()\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in val_load:\n",
    "                out = model(xb.to(device)).cpu().numpy()\n",
    "                preds.append(out); truths.append(yb.numpy())\n",
    "        val_loss = mean_squared_error(\n",
    "            np.vstack(truths), np.vstack(preds))\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss, wait = val_loss, 0\n",
    "            best_weights = model.state_dict()\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                break\n",
    "\n",
    "    # load best\n",
    "    model.load_state_dict(best_weights)\n",
    "    # evaluate on train/val/test\n",
    "    def eval_set(Xs, ys):\n",
    "        yhat = model(torch.from_numpy(Xs).float().to(device)).cpu().detach().numpy()\n",
    "        return (\n",
    "            np.sqrt(mean_squared_error(ys, yhat)),\n",
    "            r2_score(ys, yhat)\n",
    "        )\n",
    "\n",
    "    rmse_tr, r2_tr = eval_set(X_tr, y_tr)\n",
    "    rmse_val, r2_val = eval_set(X_val, y_val)\n",
    "    rmse_te, r2_te = eval_set(X_te, y_te)\n",
    "\n",
    "    return {\n",
    "      'model': model,\n",
    "      'rmse_tr': rmse_tr, 'r2_tr': r2_tr,\n",
    "      'rmse_val': rmse_val, 'r2_val': r2_val,\n",
    "      'rmse_te': rmse_te, 'r2_te': r2_te\n",
    "    }\n",
    "\n",
    "# 5) Quick grid\n",
    "configs = [\n",
    "    {'hidden_dims':[32,16], 'dropout':0.1, 'lr':1e-3},\n",
    "    {'hidden_dims':[64,32], 'dropout':0.2, 'lr':5e-4},\n",
    "]\n",
    "best = None\n",
    "for cfg in configs:\n",
    "    res = train_nn(**cfg)\n",
    "    print(cfg, res['r2_val'], res['r2_te'])\n",
    "    if best is None or res['r2_te']>best['r2_te']:\n",
    "        best = res\n",
    "\n",
    "print(\"\\nBest NN Performance:\")\n",
    "print(f\"Train R²={best['r2_tr']:.4f}, Val R²={best['r2_val']:.4f}, Test R²={best['r2_te']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite extensive experimentation, none of the advanced models surpassed the simple OLS in terms of performance and interpretability. Thus, OLS was retained as the final model for its robustness and clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5  Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Metrics: RMSE and R Square\n",
    "\n",
    "RMSE and R2 for all 6 linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_9c196\">\n",
       "  <caption>Baseline Linear-Family Models – Performance KPI Matrix</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9c196_level0_col0\" class=\"col_heading level0 col0\" >RMSE_train</th>\n",
       "      <th id=\"T_9c196_level0_col1\" class=\"col_heading level0 col1\" >R2_train</th>\n",
       "      <th id=\"T_9c196_level0_col2\" class=\"col_heading level0 col2\" >RMSE_test</th>\n",
       "      <th id=\"T_9c196_level0_col3\" class=\"col_heading level0 col3\" >R2_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"index_name level1\" >Target</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9c196_level0_row0\" class=\"row_heading level0 row0\" >LR</th>\n",
       "      <th id=\"T_9c196_level1_row0\" class=\"row_heading level1 row0\" >TCI/road_len</th>\n",
       "      <td id=\"T_9c196_row0_col0\" class=\"data row0 col0\" >5.3420</td>\n",
       "      <td id=\"T_9c196_row0_col1\" class=\"data row0 col1\" >0.7162</td>\n",
       "      <td id=\"T_9c196_row0_col2\" class=\"data row0 col2\" >6.1411</td>\n",
       "      <td id=\"T_9c196_row0_col3\" class=\"data row0 col3\" >0.6770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9c196_level0_row1\" class=\"row_heading level0 row1\" >Log–Log LR</th>\n",
       "      <th id=\"T_9c196_level1_row1\" class=\"row_heading level1 row1\" >TCI/road_len</th>\n",
       "      <td id=\"T_9c196_row1_col0\" class=\"data row1 col0\" >5.0877</td>\n",
       "      <td id=\"T_9c196_row1_col1\" class=\"data row1 col1\" >0.1728</td>\n",
       "      <td id=\"T_9c196_row1_col2\" class=\"data row1 col2\" >5.0621</td>\n",
       "      <td id=\"T_9c196_row1_col3\" class=\"data row1 col3\" >-0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9c196_level0_row2\" class=\"row_heading level0 row2\" >Poly LR (d=3)</th>\n",
       "      <th id=\"T_9c196_level1_row2\" class=\"row_heading level1 row2\" >TCI/road_len</th>\n",
       "      <td id=\"T_9c196_row2_col0\" class=\"data row2 col0\" >2.8633</td>\n",
       "      <td id=\"T_9c196_row2_col1\" class=\"data row2 col1\" >0.9185</td>\n",
       "      <td id=\"T_9c196_row2_col2\" class=\"data row2 col2\" >33.2734</td>\n",
       "      <td id=\"T_9c196_row2_col3\" class=\"data row2 col3\" >-8.4833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9c196_level0_row3\" class=\"row_heading level0 row3\" >LR</th>\n",
       "      <th id=\"T_9c196_level1_row3\" class=\"row_heading level1 row3\" >TCI</th>\n",
       "      <td id=\"T_9c196_row3_col0\" class=\"data row3 col0\" >8548952.1574</td>\n",
       "      <td id=\"T_9c196_row3_col1\" class=\"data row3 col1\" >0.7463</td>\n",
       "      <td id=\"T_9c196_row3_col2\" class=\"data row3 col2\" >9846074.6481</td>\n",
       "      <td id=\"T_9c196_row3_col3\" class=\"data row3 col3\" >0.7968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9c196_level0_row4\" class=\"row_heading level0 row4\" >Log–Log LR</th>\n",
       "      <th id=\"T_9c196_level1_row4\" class=\"row_heading level1 row4\" >TCI</th>\n",
       "      <td id=\"T_9c196_row4_col0\" class=\"data row4 col0\" >5.8183</td>\n",
       "      <td id=\"T_9c196_row4_col1\" class=\"data row4 col1\" >0.1474</td>\n",
       "      <td id=\"T_9c196_row4_col2\" class=\"data row4 col2\" >5.7425</td>\n",
       "      <td id=\"T_9c196_row4_col3\" class=\"data row4 col3\" >-0.0201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9c196_level0_row5\" class=\"row_heading level0 row5\" >Poly LR (d=3)</th>\n",
       "      <th id=\"T_9c196_level1_row5\" class=\"row_heading level1 row5\" >TCI</th>\n",
       "      <td id=\"T_9c196_row5_col0\" class=\"data row5 col0\" >2537682.5992</td>\n",
       "      <td id=\"T_9c196_row5_col1\" class=\"data row5 col1\" >0.9776</td>\n",
       "      <td id=\"T_9c196_row5_col2\" class=\"data row5 col2\" >59692127.6602</td>\n",
       "      <td id=\"T_9c196_row5_col3\" class=\"data row5 col3\" >-6.4689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b5e6229c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "#  PERFORMANCE DASHBOARD – all six baseline models\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Scoring utility\n",
    "# ------------------------------------------------------------\n",
    "def evaluate_model(model, X_tr, X_te, y_tr, y_te):\n",
    "    \"\"\"Return RMSE_train, R2_train, RMSE_test, R2_test.\"\"\"\n",
    "    y_hat_tr = model.predict(X_tr)\n",
    "    y_hat_te = model.predict(X_te)\n",
    "    rmse_tr  = np.sqrt(mean_squared_error(y_tr, y_hat_tr))\n",
    "    rmse_te  = np.sqrt(mean_squared_error(y_te, y_hat_te))\n",
    "    r2_tr    = r2_score(y_tr, y_hat_tr)\n",
    "    r2_te    = r2_score(y_te, y_hat_te)\n",
    "    return rmse_tr, r2_tr, rmse_te, r2_te\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Regenerate splits & capture metrics\n",
    "# ------------------------------------------------------------\n",
    "results = []\n",
    "\n",
    "# --- 1) Simple LR on TCI/road_len ------------------------------------------\n",
    "X = baghdad_df[features]\n",
    "y = baghdad_df['y1']                              # target already computed\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "results.append(\n",
    "    ['LR', 'TCI/road_len', *evaluate_model(lr, X_tr, X_te, y_tr, y_te)]\n",
    ")\n",
    "\n",
    "# --- 2) Log–Log LR on TCI/road_len -----------------------------------------\n",
    "X = df_ll[features]\n",
    "y = df_ll['y2']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "results.append(\n",
    "    ['Log–Log LR', 'TCI/road_len', *evaluate_model(lr_ll, X_tr, X_te, y_tr, y_te)]\n",
    ")\n",
    "\n",
    "# --- 3) Poly-deg-3 LR on TCI/road_len --------------------------------------\n",
    "poly_tmp = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly   = poly_tmp.fit_transform(baghdad_df[features])\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_poly, baghdad_df['y1'],\n",
    "                                          test_size=0.2, random_state=42)\n",
    "results.append(\n",
    "    ['Poly LR (d=3)', 'TCI/road_len', *evaluate_model(poly_lr, X_tr, X_te, y_tr, y_te)]\n",
    ")\n",
    "\n",
    "# --- 4) Simple LR on raw TCI ------------------------------------------------\n",
    "X = baghdad_df[features]\n",
    "y = baghdad_df['TCI']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "results.append(\n",
    "    ['LR', 'TCI', *evaluate_model(lr2, X_tr, X_te, y_tr, y_te)]\n",
    ")\n",
    "\n",
    "# --- 5) Log–Log LR on raw TCI ----------------------------------------------\n",
    "X = df_ll2[features]\n",
    "y = df_ll2['y5']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "results.append(\n",
    "    ['Log–Log LR', 'TCI', *evaluate_model(lr_ll2, X_tr, X_te, y_tr, y_te)]\n",
    ")\n",
    "\n",
    "# --- 6) Poly-deg-3 LR on raw TCI -------------------------------------------\n",
    "poly_tmp2 = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly2   = poly_tmp2.fit_transform(baghdad_df[features])\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_poly2, baghdad_df['TCI'],\n",
    "                                          test_size=0.2, random_state=42)\n",
    "results.append(\n",
    "    ['Poly LR (d=3)', 'TCI', *evaluate_model(poly2_lr, X_tr, X_te, y_tr, y_te)]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Executive summary table\n",
    "# ------------------------------------------------------------\n",
    "df_perf = pd.DataFrame(\n",
    "    results,\n",
    "    columns=['Model', 'Target', 'RMSE_train', 'R2_train', 'RMSE_test', 'R2_test']\n",
    ").set_index(['Model', 'Target'])\n",
    "\n",
    "display(\n",
    "    df_perf.style.format('{:.4f}')\n",
    "          .set_caption(\"Baseline Linear-Family Models – Performance KPI Matrix\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Metrics: AIC, BIC\n",
    "\n",
    "AIC, BIC and adjusted R2 for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_180c0\">\n",
       "  <caption>Complexity‐Penalized & Out‐of‐Sample Performance</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_180c0_level0_col0\" class=\"col_heading level0 col0\" >#params</th>\n",
       "      <th id=\"T_180c0_level0_col1\" class=\"col_heading level0 col1\" >AIC</th>\n",
       "      <th id=\"T_180c0_level0_col2\" class=\"col_heading level0 col2\" >BIC</th>\n",
       "      <th id=\"T_180c0_level0_col3\" class=\"col_heading level0 col3\" >R2_train</th>\n",
       "      <th id=\"T_180c0_level0_col4\" class=\"col_heading level0 col4\" >R2_adj</th>\n",
       "      <th id=\"T_180c0_level0_col5\" class=\"col_heading level0 col5\" >RMSE_test</th>\n",
       "      <th id=\"T_180c0_level0_col6\" class=\"col_heading level0 col6\" >R2_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_180c0_level0_row0\" class=\"row_heading level0 row0\" >OLS (TCI/road_len)</th>\n",
       "      <td id=\"T_180c0_row0_col0\" class=\"data row0 col0\" >27</td>\n",
       "      <td id=\"T_180c0_row0_col1\" class=\"data row0 col1\" >1403.30</td>\n",
       "      <td id=\"T_180c0_row0_col2\" class=\"data row0 col2\" >1511.27</td>\n",
       "      <td id=\"T_180c0_row0_col3\" class=\"data row0 col3\" >0.7171</td>\n",
       "      <td id=\"T_180c0_row0_col4\" class=\"data row0 col4\" >0.6967</td>\n",
       "      <td id=\"T_180c0_row0_col5\" class=\"data row0 col5\" >6.16</td>\n",
       "      <td id=\"T_180c0_row0_col6\" class=\"data row0 col6\" >0.6745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_180c0_level0_row1\" class=\"row_heading level0 row1\" >LogLog (TCI/road_len)</th>\n",
       "      <td id=\"T_180c0_row1_col0\" class=\"data row1 col0\" >30</td>\n",
       "      <td id=\"T_180c0_row1_col1\" class=\"data row1 col1\" >1364.47</td>\n",
       "      <td id=\"T_180c0_row1_col2\" class=\"data row1 col2\" >1484.44</td>\n",
       "      <td id=\"T_180c0_row1_col3\" class=\"data row1 col3\" >0.1865</td>\n",
       "      <td id=\"T_180c0_row1_col4\" class=\"data row1 col4\" >0.1209</td>\n",
       "      <td id=\"T_180c0_row1_col5\" class=\"data row1 col5\" >5.21</td>\n",
       "      <td id=\"T_180c0_row1_col6\" class=\"data row1 col6\" >-0.0668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_180c0_level0_row2\" class=\"row_heading level0 row2\" >Poly3 (TCI/road_len)</th>\n",
       "      <td id=\"T_180c0_row2_col0\" class=\"data row2 col0\" >3352</td>\n",
       "      <td id=\"T_180c0_row2_col1\" class=\"data row2 col1\" >-206.74</td>\n",
       "      <td id=\"T_180c0_row2_col2\" class=\"data row2 col2\" >13197.70</td>\n",
       "      <td id=\"T_180c0_row2_col3\" class=\"data row2 col3\" >1.0000</td>\n",
       "      <td id=\"T_180c0_row2_col4\" class=\"data row2 col4\" >1.0000</td>\n",
       "      <td id=\"T_180c0_row2_col5\" class=\"data row2 col5\" >349.31</td>\n",
       "      <td id=\"T_180c0_row2_col6\" class=\"data row2 col6\" >-1044.1444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_180c0_level0_row3\" class=\"row_heading level0 row3\" >OLS (TCI)</th>\n",
       "      <td id=\"T_180c0_row3_col0\" class=\"data row3 col0\" >27</td>\n",
       "      <td id=\"T_180c0_row3_col1\" class=\"data row3 col1\" >12917.94</td>\n",
       "      <td id=\"T_180c0_row3_col2\" class=\"data row3 col2\" >13025.91</td>\n",
       "      <td id=\"T_180c0_row3_col3\" class=\"data row3 col3\" >0.7469</td>\n",
       "      <td id=\"T_180c0_row3_col4\" class=\"data row3 col4\" >0.7286</td>\n",
       "      <td id=\"T_180c0_row3_col5\" class=\"data row3 col5\" >9826741.80</td>\n",
       "      <td id=\"T_180c0_row3_col6\" class=\"data row3 col6\" >0.7976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_180c0_level0_row4\" class=\"row_heading level0 row4\" >LogLog (TCI)</th>\n",
       "      <td id=\"T_180c0_row4_col0\" class=\"data row4 col0\" >30</td>\n",
       "      <td id=\"T_180c0_row4_col1\" class=\"data row4 col1\" >1472.57</td>\n",
       "      <td id=\"T_180c0_row4_col2\" class=\"data row4 col2\" >1592.53</td>\n",
       "      <td id=\"T_180c0_row4_col3\" class=\"data row4 col3\" >0.1617</td>\n",
       "      <td id=\"T_180c0_row4_col4\" class=\"data row4 col4\" >0.0941</td>\n",
       "      <td id=\"T_180c0_row4_col5\" class=\"data row4 col5\" >5.91</td>\n",
       "      <td id=\"T_180c0_row4_col6\" class=\"data row4 col6\" >-0.0820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_180c0_level0_row5\" class=\"row_heading level0 row5\" >Poly3 (TCI)</th>\n",
       "      <td id=\"T_180c0_row5_col0\" class=\"data row5 col0\" >3352</td>\n",
       "      <td id=\"T_180c0_row5_col1\" class=\"data row5 col1\" >10670.06</td>\n",
       "      <td id=\"T_180c0_row5_col2\" class=\"data row5 col2\" >24074.50</td>\n",
       "      <td id=\"T_180c0_row5_col3\" class=\"data row5 col3\" >1.0000</td>\n",
       "      <td id=\"T_180c0_row5_col4\" class=\"data row5 col4\" >1.0000</td>\n",
       "      <td id=\"T_180c0_row5_col5\" class=\"data row5 col5\" >323927385.85</td>\n",
       "      <td id=\"T_180c0_row5_col6\" class=\"data row5 col6\" >-218.9459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b4e59c6610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_ic(model, X_tr, y_tr):\n",
    "    \"\"\"Return (p, AIC, BIC, R2_train, R2_adj).\"\"\"\n",
    "    y_hat = model.predict(X_tr)\n",
    "    rss   = ((y_tr - y_hat) ** 2).sum()\n",
    "    n     = len(y_tr)\n",
    "    p     = np.count_nonzero(model.coef_) + 1\n",
    "    aic   = n * np.log(rss / n) + 2 * p\n",
    "    bic   = n * np.log(rss / n) + p * np.log(n)\n",
    "    r2    = r2_score(y_tr, y_hat)\n",
    "    r2_adj= 1 - (1 - r2)*(n - 1)/(n - p - 1)\n",
    "    return p, aic, bic, r2, r2_adj\n",
    "\n",
    "def compute_test_metrics(model, X_te, y_te):\n",
    "    \"\"\"Return (RMSE_test, R2_test).\"\"\"\n",
    "    y_hat = model.predict(X_te)\n",
    "    return np.sqrt(mean_squared_error(y_te, y_hat)), r2_score(y_te, y_hat)\n",
    "\n",
    "# container\n",
    "rows = []\n",
    "\n",
    "# 1) OLS on TCI/road_len\n",
    "y1 = baghdad_df['TCI'] / baghdad_df['road_len']\n",
    "X1 = baghdad_df[features]\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "rows.append(\n",
    "    ['OLS (TCI/road_len)',\n",
    "     *compute_ic(lr, X_tr, y_tr),\n",
    "     *compute_test_metrics(lr, X_te, y_te)]\n",
    ")\n",
    "\n",
    "# 2) Log–Log OLS on TCI/road_len\n",
    "X2, y2 = df_ll[features], df_ll['y2']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "rows.append(\n",
    "    ['LogLog (TCI/road_len)',\n",
    "     *compute_ic(lr_ll, X_tr, y_tr),\n",
    "     *compute_test_metrics(lr_ll, X_te, y_te)]\n",
    ")\n",
    "\n",
    "# 3) Poly3 OLS on TCI/road_len\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X3 = poly.fit_transform(baghdad_df[features])\n",
    "y3 = baghdad_df['TCI'] / baghdad_df['road_len']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X3, y3, test_size=0.2, random_state=42)\n",
    "rows.append(\n",
    "    ['Poly3 (TCI/road_len)',\n",
    "     *compute_ic(poly_lr, X_tr, y_tr),\n",
    "     *compute_test_metrics(poly_lr, X_te, y_te)]\n",
    ")\n",
    "\n",
    "# 4) OLS on TCI\n",
    "X4, y4 = baghdad_df[features], baghdad_df['TCI']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X4, y4, test_size=0.2, random_state=42)\n",
    "rows.append(\n",
    "    ['OLS (TCI)',\n",
    "     *compute_ic(lr2, X_tr, y_tr),\n",
    "     *compute_test_metrics(lr2, X_te, y_te)]\n",
    ")\n",
    "\n",
    "# 5) Log–Log OLS on TCI\n",
    "X5, y5 = df_ll2[features], df_ll2['y5']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X5, y5, test_size=0.2, random_state=42)\n",
    "rows.append(\n",
    "    ['LogLog (TCI)',\n",
    "     *compute_ic(lr_ll2, X_tr, y_tr),\n",
    "     *compute_test_metrics(lr_ll2, X_te, y_te)]\n",
    ")\n",
    "\n",
    "# 6) Poly3 OLS on TCI\n",
    "poly2 = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X6 = poly2.fit_transform(baghdad_df[features])\n",
    "y6 = baghdad_df['TCI']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X6, y6, test_size=0.2, random_state=42)\n",
    "rows.append(\n",
    "    ['Poly3 (TCI)',\n",
    "     *compute_ic(poly2_lr, X_tr, y_tr),\n",
    "     *compute_test_metrics(poly2_lr, X_te, y_te)]\n",
    ")\n",
    "\n",
    "# build DataFrame\n",
    "df_compare = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\n",
    "      'Model', '#params', 'AIC', 'BIC', 'R2_train', 'R2_adj',\n",
    "      'RMSE_test', 'R2_test'\n",
    "    ]\n",
    ").set_index('Model')\n",
    "\n",
    "# display\n",
    "display(\n",
    "    df_compare.style\n",
    "      .format({\n",
    "         '#params':'{:.0f}',\n",
    "         'AIC':'{:.2f}','BIC':'{:.2f}',\n",
    "         'R2_train':'{:.4f}','R2_adj':'{:.4f}',\n",
    "         'RMSE_test':'{:.2f}','R2_test':'{:.4f}'\n",
    "      })\n",
    "      .set_caption(\"Complexity‐Penalized & Out‐of‐Sample Performance\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across both target definitions, the simple OLS approaches (12 parameters, no transforms or high-order terms) consistently give you the best out-of-sample R² while keeping model complexity and information‐criteria penalties low.\n",
    "\n",
    "The polynomial expansions over-fit, and the log–log specs under-fit—they never beat plain OLS on real-world generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Best Model Traceback\n",
    "\n",
    "Coefficient traceback for the best-performing OLS model (Target = TCI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS coefficients in native units (ΔTCI per 1-unit feature change):\n",
      "\n",
      "                       coefficient\n",
      "feature                           \n",
      "no2_mean             53986362.3235\n",
      "Intercept            17055232.5802\n",
      "poi_count              930026.1514\n",
      "LST_day_mean          -462200.1825\n",
      "lu_retail_area            933.0909\n",
      "NTL_mean                 -602.1745\n",
      "lu_farmyard_area         -570.2696\n",
      "road_motorway_len        -375.2164\n",
      "road_trunk_len           -266.7777\n",
      "road_primary_len         -206.2958\n",
      "road_len                   57.6798\n",
      "road_residential_len      -49.7191\n",
      "road_secondary_len        -48.4863\n",
      "road_tertiary_len         -43.1960\n",
      "pop_sum_m                  22.0768\n",
      "grassland_a                -3.4462\n",
      "cropland_a                 -0.3315\n",
      "non_built_area              0.3116\n",
      "lu_commercial_area          0.3023\n",
      "built_up_a                 -0.0916\n",
      "lu_residential_area        -0.0825\n",
      "lu_industrial_area         -0.0802\n",
      "lu_farmland_area            0.0066\n"
     ]
    }
   ],
   "source": [
    "# 1. Re-create the exact train/test split used for lr2\n",
    "X = baghdad_df[features]            # <— same feature list\n",
    "y = baghdad_df['TCI']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. (Re)fit lr2 on the training fold, if not already in memory\n",
    "best_lr = LinearRegression().fit(X_tr, y_tr)\n",
    "\n",
    "# 3. Extract coefficients & intercept (already in original units)\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'coefficient': best_lr.coef_\n",
    "}).set_index('feature')\n",
    "\n",
    "# Add intercept row for completeness\n",
    "coef_df.loc['Intercept'] = best_lr.intercept_\n",
    "\n",
    "# 4. Sort by absolute magnitude for readability\n",
    "coef_df['abs_coef'] = coef_df['coefficient'].abs()\n",
    "coef_df = coef_df.sort_values('abs_coef', ascending=False).drop(columns='abs_coef')\n",
    "\n",
    "# 5. Display\n",
    "print(\"OLS coefficients in native units (ΔTCI per 1-unit feature change):\\n\")\n",
    "print(coef_df.to_string(float_format='%.4f'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
